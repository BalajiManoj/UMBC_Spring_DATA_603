# UMBC_Spring_DATA_603

# Assignment 1: Big Data (BU94693)


 Big Data with example and types
- 6 ‘V’s of Big Data (define each)
- Phases of Big Data analysis (discuss each)
- Challenges in Big Data analysis (discuss each)


## Big Data:

These datasets are numerous. The size of the data makes it difficult to store, examine, and manipulate it quickly. The world with access to the internet is dominated by big data. Millions of bytes of data are generated every second by social media platforms like Facebook, WhatsApp, and Instagram; this data must be stored so that it may be reviewed and enhanced for better use. This data can be presented in a range of media, such as pictures, videos, text, maps, and written documents. Also, we can use financial services to save transactional data.


Volume: Volume describes the amount of data that is produced and stored in a big data system.   The employment of sophisticated processing technology that is far more powerful than a standard laptop or desktop CPU is required because to the enormous volumes of data. Consider Instagram or Twitter as examples of large volume datasets. Posting photos, leaving comments, liking posts, playing games, and other activities take up a lot of time from users. There is a great deal of opportunity for analysis, pattern recognition, and other processes with these constantly growing data sets.

Variety includes the different formats and ways that different sorts of data are arranged and prepared for processing. Major brands like Facebook, Twitter, Pinterest, Google AdWords, and CRM platforms all provide data that may be gathered, saved, and then evaluated.


Velocity: The pace at which data is processed implies that there will always be more data accessible than there was before, but it also suggests that the rate at which it is processed must be equally as fast.


Value: The amount of data that we store, or process is essential, but it is not the sole factor. Also, the data must be recorded, analyzed, and assessed in order to gain insights.


Veracity: Veracity is a term used to describe how reliable and high-quality the data is. Big Data's usefulness is still undeniable if the data is not dependable and/or trustworthy. Working with real-time updated data makes this especially true. As a result, every step of the collection and processing of Big Data must include checks and balances to ensure data veracity.


Variability: The several ways that data can be structured or used might be referred to as variability.




## Phases of Big Data analysis

Data Collection and Recovery: This is the first phase, during this section of the analysis, we collect real-time data from the sources. We can collect the data from data generators including IoT devices, social media platforms, satellites, and more. Metadata is another type of data generator that consists of data information such as the location from which the data is sent, the time at which it was sent, and so on.


Information Extraction and Cleaning / Annotation: In this second phase, we extract the necessary information from data sources and present it in a structural and comprehensible format. To get this, we must clean up the data and add the values that are missing. Inconsistencies in the data can be fixed by identifying the outliers.


Data integration, aggregation, and representation is Phase 3: The data that was retrieved and cleansed from various sources in the previous stage is now combined. It is unrealistic to expect data from all sources to be homogeneous. For easier comprehension, these data can be displayed as a box plot, bar graph, histogram, or pie chart.


Query Processing, Data Modeling, and Analysis: In the 4th phase, we utilize query languages to query the data to get valuable subsets in order to understand distinct trends. A model standardizing the relationship between the data itself is created from an overall abstract of the data. The representation of data models includes database models, entity connection models, semantic data models, and others. Several languages can be used to query the data in addition to SQL.


Phase 5 is Interpretation : After developing and testing a model, the results will be understood and confirmed, assisting in decision making and identifying trends. employing visualization even.




## Challenges in Big Data analysis 

Heterogeneity and incompleteness pose the first challenge because machines assume that all data collected is homogeneous and become confused when there are variations or heterogeneity in the data. Semi-structured data must be processed before being analyzed. Even after cleaning and correcting the data, mistakes and null values may persist, and these must be addressed in the subsequent data analysis phases.


Scale is the Second Challenge- In comparison to the amount of data that is computed and reviewed, much more data is generated and retained.


Third Challenge refers to Timeliness: We anticipate accurate and prompt results from running a model. The process of gathering data, applying the rules, analyzing the results, and extracting trends takes longer for larger datasets. If the data is homogeneous and structured, the fetch time and processing time will be shorter.


Challenge 4 Privacy: On the one hand, we must secure the data, while on the other hand, data analysis is done without respect for information privacy when using data sharing. The privacy concern was also not one that needed to be overly concerned when the dataset was modest. But, as the amount of data grows, so does the concern over privacy.


Cooperation between humans and computers is a challenge 5 in large data analysis. In order to investigate the data and produce more favorable and accurate results, the model should be able to incorporate expert input and recommendations. When human analytic inputs are obtained from multiple crowd sources, there is a high possibility of mistake, uncertainty, and conflict.


## Source :
https://bau.edu/blog/characteristics-of-big-data/
